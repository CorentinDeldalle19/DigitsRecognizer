# ğŸ§  Neural Network from Scratch with NumPy

Welcome to this project where I built a **neural network from scratch**, without using any machine learning frameworks like TensorFlow or PyTorch. Just **NumPy**, lots of matrix operations, and a strong desire to understand how deep learning works under the hood ğŸ’».

---

## ğŸ“Œ Objective

The main goal of this project was to **deeply understand how a neural network functions**, including:

- How data flows through the network (**forward propagation**)
- How errors are calculated and propagated (**backpropagation**)
- How the model learns using **gradient descent**

---

## ğŸ”§ Architecture

This project implements a **3-layer neural network**:

- **Layer 1 (Input â†’ 128 neurons)**: ReLU activation  
- **Layer 2 (128 â†’ 64 neurons)**: ReLU activation  
- **Output Layer (64 â†’ 10 classes)**: Softmax activation for classification

---

## ğŸ—‚ï¸ Dataset

The model is trained on the **MNIST dataset**, a classic benchmark containing **70,000 handwritten digit images** (28x28 grayscale):

- **Training set**: 69,000 images  
- **Validation set (Dev set)**: 1,000 images

---

## âš™ï¸ Features

âœ… Forward propagation  
âœ… Backpropagation  
âœ… Training using gradient descent  
âœ… One-hot encoding of labels  
âœ… Cross-entropy loss function  
âœ… Real-time prediction and loss display  
âœ… Visual testing with custom image predictions

---

## ğŸ“Š Results

The final model is able to **accurately recognize handwritten digits**, fully implemented from scratch using only NumPy â€” no deep learning frameworks involved.